{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Intro2ML_Project3_hand_in.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dF0copX0ECwY",
    "colab_type": "text"
   },
   "source": [
    "# Project 3: Classification\n",
    "---\n",
    "\n",
    "This notebook is supposed to be used to provide the solution to the project 3 of the module Introduction to Machine Learning 2019 @ ETHZ.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTGACEsDir8B",
    "colab_type": "text"
   },
   "source": [
    "## Environmental Set-Up\n",
    "\n",
    "We first set the environment and load the later required packages, as well as fix the random seed globally."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tpfh1zCwD9ie",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import sklearn as sl\n",
    "import datetime\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "\n",
    "random_seed = 1993\n",
    "\n",
    "%matplotlib inline\n",
    "sn.set_context('notebook')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "random.seed(random_seed)\n",
    "warnings.filterwarnings('ignore')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mFStz2akJ8_",
    "colab_type": "text"
   },
   "source": [
    "After loading the basic packages, we will now install Pytorch on the virtual machine since we gonna use it to apply neural networks to solve the project as suggested. Pytorch is chosen as it provides a according to the subjective opinion of the author nice interface compared to Tensorflow, but speedwise supposingly outperforms Keras."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ff8gkZdikuU7",
    "colab_type": "code",
    "outputId": "9ff6496e-2128-45a1-ca01-161ff2c62f02",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557303663937,
     "user_tz": -120,
     "elapsed": 7687,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    }
   },
   "source": [
    "!pip3 install pandas==0.24.2\n",
    "!pip3 install torch==1.0.1"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjpIhHC0lmaT",
    "colab_type": "text"
   },
   "source": [
    "Since the Google Colab platform offers us a GPU, we will make sure to tell pytorch to use it, as it will speed up the training of our neural network significantly. Unfortunately up until now Pytorch does not support the use of TPUs (Google Colab would offer those as well)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yU20AfcYmEWd",
    "colab_type": "code",
    "outputId": "44355dbc-dc75-4952-ecf8-f6007f9f8a60",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557303664141,
     "user_tz": -120,
     "elapsed": 7886,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.utils.data as data_utils\n",
    "use_cuda = True\n",
    "\n",
    "torch.manual_seed(1993)"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvSSDj_uEqJi",
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "\n",
    "## Load in the data\n",
    "\n",
    "We now use the Google Colab API to load the data and the sample submission from disk into the temproray cloud storage attached to this PaaS (platform as a service) solution to make it accessible."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aB-sY89zEwV1",
    "colab_type": "code",
    "colab": {
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "ok": true,
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "status": 200,
       "status_text": ""
      }
     },
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "outputId": "6a98ae2c-180e-44f0-f1b7-cd162cecc70c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557303964184,
     "user_tz": -120,
     "elapsed": 307928,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    }
   },
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3nPfrvIjniL"
   },
   "source": [
    "\n",
    "---\n",
    "## Project 3\n",
    "\n",
    "The following section now solves the project 3 of the Introduction to Machine Learning course 2019.\n",
    "\n",
    "---\n",
    "\n",
    "### Formatting the data\n",
    "\n",
    "Although the data is loaded we format it to have it in the handy pandas data frame format."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "outputId": "456f5c47-4bc4-4a92-8328-a54d055c58bb",
    "id": "yn_uxXJyjniC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557303964706,
     "user_tz": -120,
     "elapsed": 308449,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    }
   },
   "source": [
    "# Get train data\n",
    "train = pd.read_hdf(\"train.h5\", \"train\")\n",
    "train.head()"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U89QqTCwtYhW",
    "colab_type": "code",
    "outputId": "ddc6c5ec-803b-402c-eeea-6f7f3d383ede",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557303964708,
     "user_tz": -120,
     "elapsed": 308450,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    }
   },
   "source": [
    "# Get the test data\n",
    "test = pd.read_hdf(\"test.h5\", \"test\")\n",
    "test.head()"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MPbHzS-Pjnh_"
   },
   "source": [
    "We quickly inspect the shape of the data to make sure the data has been correctly loaded and casted into a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5n7xncYhtm2Z",
    "colab_type": "code",
    "outputId": "781a77db-d2b8-44ac-c300-5c0cb391d29c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557303964709,
     "user_tz": -120,
     "elapsed": 308451,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    }
   },
   "source": [
    "print(\"train shape: \", np.array(train).shape)\n",
    "print(\"test shape: \", np.array(test).shape)"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "outputId": "4a4b1612-3a10-4f0b-a24e-9ef5cdb65e76",
    "id": "RZJWfUTijniH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557303964711,
     "user_tz": -120,
     "elapsed": 308452,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    }
   },
   "source": [
    "'''\n",
    "Get sample prediction file format.\n",
    "Sample predictions will be simply replaced with the ones obtained from the\n",
    "custom model.\n",
    "''' \n",
    "\n",
    "submission = pd.read_csv('sample.csv', index_col=0, float_precision='high')\n",
    "submission.head()"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZzcpbIP5jnh1"
   },
   "source": [
    "That looks very good. We seperate the label from the features for the sake of handiness of our implementations and data handling in the following."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "Obvw7tikjnhs",
    "colab": {}
   },
   "source": [
    "X_train = train.iloc[:, 1:]\n",
    "y_train = train.iloc[:, 0]"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsbJ9914k35E",
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "\n",
    "### Exploratory Data Analysis\n",
    "\n",
    "Before starting with trying to model the data, we will have a first look at the data. First we will look at the distribution of the labels in the training data, since knowing if the data is balanced or not heavily influences the choice of algorithms we will consider later on."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2cG63Oi_mqVj",
    "colab_type": "code",
    "outputId": "73610f07-be05-44e7-e5b3-cdc169b6d992",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557303965111,
     "user_tz": -120,
     "elapsed": 308850,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    }
   },
   "source": [
    "n, bins, patches = plt.hist(np.array(y_train), [-0.25, 0.25, 0.75,1.25, 1.75, \n",
    "                                                2.25, 2.75, 3.25, 3.75, 4.25],\n",
    "                            facecolor='b', alpha=0.75, align=\"mid\")\n",
    "\n",
    "\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Rel. Frequency')\n",
    "plt.title('Histogram of Class Labels in the Training Data')\n",
    "plt.axis([-0.5, 4.5, 0, 15000])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikH2x99zyYr1",
    "colab_type": "text"
   },
   "source": [
    "We see that the number of training samples we have for each class is differs quite significantly for the different classes, for instance we have roughly 3 times as many samples for class 1 as we have for class 0 or 4. We should keep that in mind as it might negatively influence the performance of our classifier especially with respect to the minority classes. If we see a severe such behavior we can cosider undersampling approaches or other techniques to overcome that obstacle. For now however, we will for simplicity proceed as if the class imbalance is no severe issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_EGIaw7vzpI",
    "colab_type": "text"
   },
   "source": [
    "Although it might be not that informative given the relative high number of features, let us quickly inspect the correlation structure of the features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aXeaK65bu-7T",
    "colab_type": "code",
    "outputId": "1a6d4c1e-38b4-4518-8629-ea997f89d6f1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557304761943,
     "user_tz": -120,
     "elapsed": 8827,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    }
   },
   "source": [
    "corr = X_train.corr()\n",
    "print(X_train.shape)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.set_title(\"Heatmap of the correlation structure\")\n",
    "sn.heatmap(\n",
    "    corr,\n",
    "    mask=np.zeros_like(corr, dtype=np.bool),\n",
    "    cmap=sn.diverging_palette(220, 10, as_cmap=True),\n",
    "    square=True,\n",
    "    ax=ax)\n",
    "plt.subplots_adjust(bottom=0.25)\n",
    "plt.show()"
   ],
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plKis-tSzKik",
    "colab_type": "text"
   },
   "source": [
    "At first sight it seems that we have rather strong correlations but at least no 1-to-1 mappings of the individual features. We will confirm this by looking at the 10 feature-pairs that are the most correlated."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "znEFR3Rr2l3K",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df, n=5):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QeW-bKgo3LY-",
    "colab_type": "code",
    "outputId": "a0e3a4ad-36c0-4da4-b397-efac2fb65b17",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557304769273,
     "user_tz": -120,
     "elapsed": 16120,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    }
   },
   "source": [
    "print(\"Top Absolute Correlations\")\n",
    "print(get_top_abs_correlations(X_train, 10))"
   ],
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS_DcQqD3Yeb",
    "colab_type": "text"
   },
   "source": [
    "As indicated by our heat map we see tthat we have no correlation of 1 and hence for now no reason to exclude any features from the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Jmx_9T6w-NJ",
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "\n",
    "### Initial Experiments: 5 Layer Pytorch NN\n",
    "\n",
    "Hereinafter, we will set up a basic 5-layer feed-forward neural network using the Pytorch framework to classify the individual data points based on the 120 features. This network is obviously simplistic and not tuned, but serves as a starting point and an easy way to familiarize oneself with the framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF0mm4jv3uQS",
    "colab_type": "text"
   },
   "source": [
    "1. Let us first transform the data in a format that is compatible with the pytorch framework and hence enables us later to fit the network to the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6Q3PVvCs_lwG",
    "colab_type": "code",
    "outputId": "4a19a549-b278-4dc8-b064-d6385d9a8dcf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557304769274,
     "user_tz": -120,
     "elapsed": 16095,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "# check if GPU is available and set the device accordingly\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')  \n",
    "torch.cuda.get_device_name(0)"
   ],
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2o774ly7oUnW",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "train_tensors = data_utils.TensorDataset(torch.cuda.FloatTensor(np.array(X_train)), torch.cuda.LongTensor(np.array(y_train)))\n",
    "train_loader = data_utils.DataLoader(train_tensors, batch_size = 200, shuffle = True)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNCjbeKdyNA2",
    "colab_type": "text"
   },
   "source": [
    "2. Let us formally define the network. We will use a 5-layer structure with ReLu activation in the hidden layers and a softmax activation at the output layer. While the number of hidden units for the input and output layer is defined by the number of features or classes respectively, we will use for our simplistic approach 80 as the number of hidden units for each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Atte4YDExSRP",
    "colab_type": "code",
    "outputId": "81a3981b-ea5c-469d-b32b-0d953c6a0c70",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557304769277,
     "user_tz": -120,
     "elapsed": 16048,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    }
   },
   "source": [
    "class SimpleNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "      super(SimpleNet, self).__init__()\n",
    "      self.fc1 = nn.Linear(120, 128)\n",
    "      self.fc2 = nn.Linear(128, 64)\n",
    "      self.fc3 = nn.Linear(64,32)\n",
    "      self.fc4 = nn.Linear(32, 16)\n",
    "      self.fc5 = nn.Linear(16, 5)\n",
    "      self.dropout = nn.Dropout(0.6)\n",
    "      #self.smax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = F.relu(self.fc1(x))\n",
    "      x = self.dropout(x)\n",
    "      x = F.relu(self.fc2(x))\n",
    "      x = self.dropout(x)\n",
    "      x = F.relu(self.fc3(x))\n",
    "      x = self.dropout(x)\n",
    "      x = F.relu(self.fc4(x))\n",
    "      x = self.dropout(x)\n",
    "      x = self.fc5(x)\n",
    "      x = F.log_softmax(x)\n",
    "      return x\n",
    "\n",
    "\n",
    "snet = SimpleNet()\n",
    "snet = snet.cuda()\n",
    "snet = snet.to(device)\n",
    "print(snet)"
   ],
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McdZoQeX2DDq",
    "colab_type": "text"
   },
   "source": [
    "3. We now define the loss function and the optimizer we intend to use."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Be7z8zja2IGS",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(snet.parameters(), lr=learning_rate, momentum=0.9)\n",
    "criterion = nn.NLLLoss()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jm6h-7uq3oGO",
    "colab_type": "text"
   },
   "source": [
    "4. Finally let us train the model, since we have set up anything."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1rDhqYCA5Jt_",
    "colab_type": "code",
    "outputId": "923e60d9-e4cb-46ea-e6da-876ad2e4cdc4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557304782447,
     "user_tz": -120,
     "elapsed": 29179,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    }
   },
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "epochs=20\n",
    "log_interval=1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "      data, target = Variable(data).to(device), Variable(target).to(device)\n",
    "      optimizer.zero_grad()\n",
    "      net_out = snet(data)\n",
    "      loss = criterion(net_out, target)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      if batch_idx % log_interval == 0:\n",
    "         print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.data.item()))"
   ],
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xksWbw9RXg2u",
    "colab_type": "text"
   },
   "source": [
    "Let us now get the predictions and submit them to get a first idea of our performance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "esF6BW8YXmRf",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "test_set = torch.from_numpy(np.array(test))\n",
    "preds = []\n",
    "\n",
    "for value in test_set:\n",
    "  # then put it on the GPU, make it float and insert a fake batch dimension\n",
    "  test_value = Variable(value.cuda())\n",
    "  test_value = test_value.float()\n",
    "  test_value = test_value.unsqueeze(0)\n",
    "\n",
    "  # pass it through the model\n",
    "  prediction = snet(test_value)\n",
    "\n",
    "# get the result out and reshape it\n",
    "  cpu_pred = prediction.cpu()\n",
    "  result = np.argmax(cpu_pred.data.numpy())\n",
    "  preds.append(result)\n",
    "  \n",
    "preds = np.array(preds)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ec1YFqLAaDM5",
    "colab_type": "code",
    "outputId": "4b5fd305-43a7-418e-e85e-24cb3ea0bcac",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557304786665,
     "user_tz": -120,
     "elapsed": 33375,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "preds"
   ],
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKc6pXuISI_b",
    "colab_type": "text"
   },
   "source": [
    "We see that we only predict the majority classes. That it is not satisfactory.\n",
    "\n",
    "---\n",
    "\n",
    "### More Sophisticated Experiments: 4-Layer ANN\n",
    "\n",
    "We will now perform some more sophisticated trials to get a better performance.\n",
    "In particular we did not watch the performance of our network with respect to the desired metric, nor got a less biased estimate by monitoring any performance on a validation set.\n",
    "\n",
    "We will do so in order to get a better idea of what might be issues of our configuration. To this end we generically implement a couple function that will do the job and use the following as an inspiration: https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#model-training-and-validation-code ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aA_4RRt3W43h",
    "colab_type": "text"
   },
   "source": [
    "1. We will split our loaded data into a training and validation set. The former will be used for training purposes, while the latter will be used to monitor the performance of our network. We have seen several times that performance estimate we get on the training set will be too optimistic and should thus in favor of the estimate based on the validation set not used for model tuning purposes. The sci-kit learn library provides all we need to realize the split. We then transform the data in such a way that it fits in the pytorch framework."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5Wnynh_JpEwn",
    "colab_type": "code",
    "outputId": "4c8cf348-edeb-4e12-c897-5e74d1aa4a26",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557304786667,
     "user_tz": -120,
     "elapsed": 33365,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    }
   },
   "source": [
    "# Create train val split and create the data loader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler().fit(X_train)\n",
    "X_train_standardized = sc.transform(X_train)\n",
    "\n",
    "data_train, data_val, label_train, label_val = train_test_split(X_train_standardized, \n",
    "                                                                y_train, test_size = 0.175, \n",
    "                                                                random_state=1993)\n",
    "print(np.array(data_train).shape)\n",
    "print(np.array(label_train).shape)\n",
    "print(np.array(data_val).shape)\n",
    "print(np.array(label_val).shape)\n",
    "\n",
    "# Note that from here on we expect GPU to be available, if that is not the case \n",
    "# use torch.xxxTensor instead of float.cuda.xxxTensor\n",
    "\n",
    "train_tensors = data_utils.TensorDataset(\n",
    "    torch.cuda.FloatTensor(np.array(data_train)), \n",
    "    torch.cuda.LongTensor(np.array(label_train)))\n",
    "\n",
    "train_loader = data_utils.DataLoader(train_tensors, \n",
    "                                     batch_size = 256, shuffle = True)\n",
    "\n",
    "val_tensors = data_utils.TensorDataset(\n",
    "    torch.cuda.FloatTensor(np.array(data_val)), \n",
    "    torch.cuda.LongTensor(np.array(label_val)))\n",
    "\n",
    "val_loader = data_utils.DataLoader(val_tensors, \n",
    "                                   batch_size = 256, shuffle = True)\n",
    "\n",
    "data_loaders_dict = {'train':train_loader, 'val':val_loader}\n",
    "data_loaders_dict"
   ],
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ0guH1EgIlh",
    "colab_type": "text"
   },
   "source": [
    "2. Now we define the function to train a preset torch.nn model and thereby monitor the performance of it. This is again inspired by the previously referenced official pytorch tutorial. The nice thing about this function is that it will allow us to monitor the model performance on both the training and validation set at each epoch and will return the best found model i.e. the one with the highest validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q60JYAmZSjqD",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=100):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "                labels = labels.type(torch.LongTensor).to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                  # Get model outputs and calculate loss\n",
    "                  outputs = model(inputs)\n",
    "                  loss = criterion(outputs, labels)\n",
    "                  _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                  loss.backward()\n",
    "                  optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.6f} Acc: {:.6f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model if it has the best val accurary\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, \n",
    "                                                        time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiQQTXt3hFiM",
    "colab_type": "text"
   },
   "source": [
    "3. We now set up the model i.e. the network structure. We will use a very basic model that consists only of two hidden layers, but sufficiently many neurons in each of those to resemble different transformation of the input features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-TpU5sd0hE2o",
    "colab_type": "code",
    "outputId": "173fc7b5-b378-424c-9907-5cb30e0ab3aa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557307654989,
     "user_tz": -120,
     "elapsed": 800,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    }
   },
   "source": [
    "class EasyNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "      super(EasyNet, self).__init__()\n",
    "      self.fc1 = nn.Linear(120, 1024)\n",
    "      self.fc2 = nn.Linear(1024, 256)\n",
    "      self.fc3 = nn.Linear(256,5)\n",
    "      self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = F.relu(self.fc1(x))\n",
    "      x = self.dropout(x)\n",
    "      x = F.relu(self.fc2(x))\n",
    "      x = self.dropout(x)\n",
    "      x = self.fc3(x)\n",
    "      return x\n",
    "\n",
    "\n",
    "enet = EasyNet()\n",
    "enet = enet.cuda()\n",
    "enet = enet.to(device)\n",
    "print(enet)"
   ],
   "execution_count": 85,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlok2gNzhrxB",
    "colab_type": "text"
   },
   "source": [
    "4. We now set up the optimizer and the criterion. We thereby use the Adam optimizer as it automatically adapts the learning rate. Since our outputlayer so far consists of five neurons outputting values in $\\mathbb{R}$, we will use the CrossEntropyLoss criterion provided by pytorch. Additionally we set the weight_decay to 1e-5 to have a equivalently high weight regularization according to the $l_2$-norm to prevent overfitting. Note that we also used Dropout layers in our network structure for the same purpose."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8vrsGUpIhvNa",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "params_to_update = enet.parameters()\n",
    "\n",
    "optimizer_ft = optim.Adam(params_to_update, lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(size_average=True)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MVvs9mf6H0H",
    "colab_type": "text"
   },
   "source": [
    "Before we start training our model, let us quickly as a last control mechanism check if the split of the train and validation split was done such that the distribution of the class labels is roughly the same in both."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OUqIX8Gq6TaJ",
    "colab_type": "code",
    "outputId": "021d3585-df69-481c-ba59-8fc15f051ede",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557305381697,
     "user_tz": -120,
     "elapsed": 671,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    }
   },
   "source": [
    "counts_train = np.unique(label_train, return_counts=True)[1]\n",
    "counts_val = np.unique(label_val, return_counts=True)[1]\n",
    "(counts_train, counts_val)"
   ],
   "execution_count": 68,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9itwU_36U3Q",
    "colab_type": "text"
   },
   "source": [
    "That is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71FLygeZEpOO",
    "colab_type": "text"
   },
   "source": [
    "5. Having done all of this we are good to go and can train our model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R3p5POrtE9ri",
    "colab_type": "code",
    "outputId": "dfdb041d-d7a0-4e9e-e8fd-3e9dabfd90e1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557307784002,
     "user_tz": -120,
     "elapsed": 121681,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17051
    }
   },
   "source": [
    "num_epochs=200\n",
    "enet_fit, hist = train_model(enet, data_loaders_dict, \n",
    "                             criterion, optimizer_ft, \n",
    "                             num_epochs=num_epochs)"
   ],
   "execution_count": 87,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usgRO8V8D5I9",
    "colab_type": "text"
   },
   "source": [
    "6. The validation accuracy looks promosing. However to get an idea of how representative that score is for the performance on the test set we will now predict the values for the test set and create a submission from those predictions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3JleaT-WrLA3",
    "colab_type": "code",
    "outputId": "c05de50e-a092-4750-91ce-0e57f75401fb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557305124174,
     "user_tz": -120,
     "elapsed": 304378,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "test_standardized = sc.transform(test)\n",
    "test_set = torch.from_numpy(np.array(test_standardized))\n",
    "predictions = []\n",
    "\n",
    "for value in test_set:\n",
    "  # then put it on the GPU, make it float and insert a fake batch dimension\n",
    "  test_value = Variable(value.cuda())\n",
    "  test_value = test_value.float()\n",
    "  test_value = test_value.unsqueeze(0)\n",
    "\n",
    "  # pass it through the model\n",
    "  outputs = enet_fit(test_value)\n",
    "  _, preds = torch.max(outputs, 1)\n",
    "\n",
    "# get the result out and reshape it\n",
    "  predictions.append(preds.cpu().numpy()[0])\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "predictions"
   ],
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QXoPXHoszO2",
    "colab_type": "text"
   },
   "source": [
    "Let us quickly check the fitted distribution of the class labels."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gv0Mh52SrVje",
    "colab_type": "code",
    "outputId": "59009364-38c6-47cd-d2f8-563d1b746df3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557305124175,
     "user_tz": -120,
     "elapsed": 304366,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "unique, counts = np.unique(predictions, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ],
   "execution_count": 55,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMDMAawFtFmS",
    "colab_type": "text"
   },
   "source": [
    "Well that looks quite nice, recalling the distribution of the labels of our training and validation data. As we assume that the data for which we predicted the labels comes from the same distribution as the one which generated our training and validation data, this is exactly what we would expect.\n",
    "\n",
    "Let us now finally create a submission based on those predictions and download it, such that we potentially could hand it in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdXG2idCxHFm",
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "### Create submission"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Yor2Bbzsafyq",
    "colab_type": "code",
    "outputId": "a69f945d-0069-4e8c-d241-d4e6af9327be",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1557305124176,
     "user_tz": -120,
     "elapsed": 304345,
     "user": {
      "displayName": "Daniel Paysan",
      "photoUrl": "https://lh4.googleusercontent.com/-HjRiH1Pwbu0/AAAAAAAAAAI/AAAAAAAAAA8/hD4qoVYmxLY/s64/photo.jpg",
      "userId": "00174475314057917185"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    }
   },
   "source": [
    "submission['y'] = predictions\n",
    "submission.head()"
   ],
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wD_P3gKvc9NG",
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "\n",
    "## Export data\n",
    "\n",
    "We finally use the Google Colab API to download our submission data frame in from of an csv, that we can submit to the submission platform."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WNhRwnu7djcg",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from google.colab import files\n",
    "\n",
    "ts = str(datetime.datetime.utcnow())\n",
    "ts = ts.replace(' ', '_')\n",
    "Filename = 'ANN_hand_in' #@param {type:\"string\"}\n",
    "fname = Filename+ts+'.csv'\n",
    "\n",
    "with open(fname, 'w') as f:\n",
    "  submission.to_csv(f, float_format='%.64f', index=True, header=True)\n",
    "\n",
    "files.download(fname)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HLkSFesU77_P",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}