{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Intro2ML_Project4_handin.ipynb",
   "provenance": [
    {
     "file_id": "1KzMIc6jz0wf0Jk1TXWYXHgQ5vFCPlZ7A",
     "timestamp": 1559314708119
    },
    {
     "file_id": "1D2rSVkSWHryxo5jEzBXlMYe0rVLX06AK",
     "timestamp": 1557307832051
    }
   ],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dF0copX0ECwY",
    "colab_type": "text"
   },
   "source": [
    "# Project 4: Open-ended\n",
    "---\n",
    "\n",
    "This notebook is supposed to be used to provide the solution to the project 4 of the module Introduction to Machine Learning 2019 @ ETHZ.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTGACEsDir8B",
    "colab_type": "text"
   },
   "source": [
    "## Environmental Set-Up\n",
    "\n",
    "We first set the environment and load the later required packages, as well as fix the random seed globally."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tpfh1zCwD9ie",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import sklearn as sl\n",
    "import datetime\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "\n",
    "random_seed = 2208\n",
    "\n",
    "%matplotlib inline\n",
    "sn.set_context('notebook')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "random.seed(random_seed)\n",
    "warnings.filterwarnings('ignore')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mFStz2akJ8_",
    "colab_type": "text"
   },
   "source": [
    "After loading the basic packages, we will now install Pytorch on the virtual machine since we gonna use it to apply neural networks to solve the project as suggested. Pytorch is chosen as it provides a according to the subjective opinion of the author nice interface compared to Tensorflow, but speedwise supposingly outperforms Keras."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ff8gkZdikuU7",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "!pip3 install pandas==0.24.2\n",
    "!pip3 install torch"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjpIhHC0lmaT",
    "colab_type": "text"
   },
   "source": [
    "Since the Google Colab platform offers us a GPU, we will make sure to tell pytorch to use it, as it will speed up the training of our neural network significantly. Unfortunately up until now Pytorch does not support the use of TPUs (Google Colab would offer those as well)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yU20AfcYmEWd",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.utils.data as data_utils\n",
    "use_cuda = True\n",
    "\n",
    "torch.manual_seed(2208)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvSSDj_uEqJi",
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "\n",
    "## Load in the data\n",
    "\n",
    "We now use the Google Colab API to load the data and the sample submission from disk into the temproray cloud storage attached to this PaaS (platform as a service) solution to make it accessible."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aB-sY89zEwV1",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9nPeN25oWWy",
    "colab_type": "text"
   },
   "source": [
    "Alternatively one can connect Colab with Google Drive for larger data transmission rates."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2m7F_u6hod5F",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "!pip install -U -q PyDrive\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j0JtVvO0pUOT",
    "colab_type": "code",
    "colab": {}
   },
   "source": [],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3nPfrvIjniL"
   },
   "source": [
    "\n",
    "---\n",
    "## Project 4\n",
    "\n",
    "The following section now solves the project 4 of the Introduction to Machine Learning course 2019.\n",
    "\n",
    "---\n",
    "\n",
    "### Formatting the data\n",
    "\n",
    "Although the data is loaded we format it to have it in the handy pandas data frame format."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "yn_uxXJyjniC",
    "colab": {}
   },
   "source": [
    "# Get the labeled train data\n",
    "train_labeled = pd.read_hdf(\"train_labeled.h5\", \"train\")\n",
    "train_labeled.describe()\n",
    "\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qT2FQi1uPGzy",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Get the unlabeled train data\n",
    "train_unlabeled = pd.read_hdf(\"train_unlabeled.h5\", \"train\")\n",
    "train_unlabeled.head()\n",
    "\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0E_XrxOqPJUS",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Get the test data\n",
    "test = pd.read_hdf(\"test.h5\", \"test\")\n",
    "test.head()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MPbHzS-Pjnh_"
   },
   "source": [
    "We quickly inspect the shape of the data to make sure the data has been correctly loaded and casted into a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5n7xncYhtm2Z",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "print(\"train labeled shape: \", np.array(train_labeled).shape)\n",
    "print(\"train unlabeled shape: \", np.array(train_unlabeled).shape)\n",
    "print(\"test shape: \", np.array(test).shape)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZzcpbIP5jnh1"
   },
   "source": [
    "That looks very good. We seperate the label from the features for the sake of handiness of our implementations and data handling in the following."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "Obvw7tikjnhs",
    "colab": {}
   },
   "source": [
    "X_train_labeled = train_labeled.iloc[:, 1:]\n",
    "y_train_labeled = train_labeled.iloc[:, 0]"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "RZJWfUTijniH",
    "colab": {}
   },
   "source": [
    "'''\n",
    "Get sample prediction file format.\n",
    "Sample predictions will be simply replaced with the ones obtained from the\n",
    "custom model.\n",
    "''' \n",
    "\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V0u7YLPHPQF_",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "submission = pd.read_csv('sample.csv', index_col=0, float_precision='high')\n",
    "submission.head()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsbJ9914k35E",
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "\n",
    "### Exploratory Data Analysis\n",
    "\n",
    "Before starting with trying to model the data, we will have a first look at the data. First we will look at the distribution of the labels in the training data, since knowing if the data is balanced or not heavily influences the choice of algorithms we will consider later on."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2cG63Oi_mqVj",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "n, bins, patches = plt.hist(np.array(y_train_labeled),\n",
    "                            facecolor='b', alpha=0.75, align=\"mid\")\n",
    "\n",
    "\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Rel. Frequency')\n",
    "plt.title('Histogram of Class Labels in the Training Data')\n",
    "plt.axis([0, 9, 0, 2000])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i8_QWreD8xLG",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train_std = StandardScaler().fit_transform(X_train_labeled)\n",
    "U,s,Vt = randomized_svd(np.array(X_train_std), n_components=139)\n",
    "\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wFD1OrNcBaBz",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "plt.plot(s**2)\n",
    "plt.ylabel('Eigenvalue spectrum')\n",
    "plt.show()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikH2x99zyYr1",
    "colab_type": "text"
   },
   "source": [
    "We see that the number of training samples we have for each class is about the same. So problems arising from class imbalance seem to be not likely for the given task and we will not consider special approaches oriented towards adressing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_EGIaw7vzpI",
    "colab_type": "text"
   },
   "source": [
    "Although it might be not that informative given the relative high number of features, let us quickly inspect the correlation structure of the features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aXeaK65bu-7T",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "corr = X_train_labeled.corr()\n",
    "print(X_train_labeled.shape)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.set_title(\"Heatmap of the correlation structure\")\n",
    "sn.heatmap(\n",
    "    corr,\n",
    "    mask=np.zeros_like(corr, dtype=np.bool),\n",
    "    cmap=sn.diverging_palette(220, 10, as_cmap=True),\n",
    "    square=True,\n",
    "    ax=ax)\n",
    "plt.subplots_adjust(bottom=0.25)\n",
    "plt.show()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plKis-tSzKik",
    "colab_type": "text"
   },
   "source": [
    "At first sight it seems that we have rather strong correlations but at least no 1-to-1 mappings of the individual features. We will confirm this by looking at the 10 feature-pairs that are the most correlated."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "znEFR3Rr2l3K",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df, n=5):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QeW-bKgo3LY-",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "print(\"Top Absolute Correlations\")\n",
    "print(get_top_abs_correlations(X_train_labeled, 10))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS_DcQqD3Yeb",
    "colab_type": "text"
   },
   "source": [
    "As indicated by our heat map we see tthat we have no correlation of 1 and hence for now no reason to exclude any features from the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKc6pXuISI_b",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### More Sophisticated Experiments: 4-Layer ANN\n",
    "\n",
    "After trying out a number of different approaches, including combinations of dimensionality reduction and classification approachs like NNMF and SVMs, many semi-supervised approaches such as Ladder networks and beta-VAEs, we will go one step back and will perform sophisticated trials a NN fitted to the labeled data only. to get a better performance.\n",
    "To this end we use methods from the following resource: https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#model-training-and-validation-code .\n",
    "\n",
    "The actual procedure is explained in the following."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BkRMs4cTulln",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# check if GPU is available and set the device accordingly\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')  \n",
    "torch.cuda.get_device_name(0)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aA_4RRt3W43h",
    "colab_type": "text"
   },
   "source": [
    "1. We will split our loaded data into a training and validation set. The former will be used for training purposes, while the latter will be used to monitor the performance of our network. We have seen several times that performance estimate we get on the training set will be too optimistic and should thus in favor of the estimate based on the validation set not used for model tuning purposes. The sci-kit learn library provides all we need to realize the split. We then transform the data in such a way that it fits in the pytorch framework."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5Wnynh_JpEwn",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Create train val split and create the data loader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler().fit(np.concatenate((np.array(X_train_labeled), np.array(train_unlabeled))))\n",
    "X_train_standardized = sc.transform(X_train_labeled)\n",
    "\n",
    "data_train, data_val, label_train, label_val = train_test_split(X_train_standardized, \n",
    "                                                                y_train_labeled, test_size = 0.15, \n",
    "                                                                random_state=2208)\n",
    "print(np.array(data_train).shape)\n",
    "print(np.array(label_train).shape)\n",
    "print(np.array(data_val).shape)\n",
    "print(np.array(label_val).shape)\n",
    "\n",
    "# Note that from here on we expect GPU to be available, if that is not the case \n",
    "# use torch.xxxTensor instead of torch.cuda.xxxTensor\n",
    "\n",
    "train_tensors = data_utils.TensorDataset(\n",
    "    torch.cuda.FloatTensor(np.array(data_train)), \n",
    "    torch.cuda.LongTensor(np.array(label_train)))\n",
    "\n",
    "train_loader = data_utils.DataLoader(train_tensors, \n",
    "                                     batch_size = 32, shuffle = True)\n",
    "\n",
    "val_tensors = data_utils.TensorDataset(\n",
    "    torch.cuda.FloatTensor(np.array(data_val)), \n",
    "    torch.cuda.LongTensor(np.array(label_val)))\n",
    "\n",
    "val_loader = data_utils.DataLoader(val_tensors, \n",
    "                                   batch_size = 32, shuffle = True)\n",
    "\n",
    "data_loaders_dict = {'train':train_loader, 'val':val_loader}\n",
    "data_loaders_dict"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ0guH1EgIlh",
    "colab_type": "text"
   },
   "source": [
    "2. Now we define the function to train a preset torch.nn model and thereby monitor the performance of it. This is again inspired by the previously referenced official pytorch tutorial. The nice thing about this function is that it will allow us to monitor the model performance on both the training and validation set at each epoch and will return the best found model i.e. the one with the highest validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q60JYAmZSjqD",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=100):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "                labels = labels.type(torch.LongTensor).to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                  # Get model outputs and calculate loss\n",
    "                  outputs = model(inputs)\n",
    "                  loss = criterion(outputs, labels)\n",
    "                  _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                  loss.backward()\n",
    "                  optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.6f} Acc: {:.6f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model if it has the best val accurary\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, \n",
    "                                                        time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiQQTXt3hFiM",
    "colab_type": "text"
   },
   "source": [
    "3. We now set up the model i.e. the network structure. We will use a very basic model that consists only of two hidden layers, but sufficiently many neurons in each of those to resemble different transformation of the input features. We also write a quick function to initialize our weights with a xavier_uniform distribution as suggested in recent literature."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "azPeMaNiuvXD",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def init_weights(m):\n",
    "  if type(m) == nn.Linear:\n",
    "    torch.nn.init.xavier_uniform(m.weight)\n",
    "    m.bias.data.fill_(0.01)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gd0pGbpq0Sz9",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "seqnet = nn.Sequential(nn.Linear(139, 512), nn.BatchNorm1d(512), nn.LeakyReLU(), nn.Dropout(0.6), \n",
    "                       nn.Linear(512, 256), nn.BatchNorm1d(256), nn.LeakyReLU(), nn.Dropout(0.5),\n",
    "                       nn.Linear(256, 256), nn.BatchNorm1d(256), nn.LeakyReLU(), nn.Dropout(0.5),\n",
    "                       nn.Linear(256, 256), nn.BatchNorm1d(256), nn.LeakyReLU(), nn.Dropout(0.4),\n",
    "                      nn.Linear(256, 10))\n",
    "seqnet.apply(init_weights)\n",
    "seqnet.cuda()\n",
    "seqnet.to(device)\n",
    "print(seqnet)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlok2gNzhrxB",
    "colab_type": "text"
   },
   "source": [
    "4. We now set up the optimizer and the criterion. We thereby use the AdamW optimizer as it automatically adapts the learning rate and has been shown to perform superior to the Adam optimizer. We use an existing implementation as given in the following : https://github.com/egg-west/AdamW-pytorch\n",
    "So far our output layer consists of 10 neurons outputting values in $\\mathbb{R}$, we will use the CrossEntropyLoss criterion provided by pytorch. Additionally we apply batch normalization as it has been shown that this improves the speed of convergence of the network. We also used dropout layers in our network structure to prevent overfitting and make our model more robust against noise."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rpcDfnrDxd9p",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    \"\"\"Implements Adam algorithm.\n",
    "    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
    "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
    "    .. _Adam\\: A Method for Stochastic Optimization:\n",
    "        https://arxiv.org/abs/1412.6980\n",
    "    .. _On the Convergence of Adam and Beyond:\n",
    "        https://openreview.net/forum?id=ryQu7f-RZ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(AdamW, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # if group['weight_decay'] != 0:\n",
    "                #     grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                # p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "                p.data.add_(-step_size,  torch.mul(p.data, group['weight_decay']).addcdiv_(1, exp_avg, denom) )\n",
    "\n",
    "        return loss"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKbJfGXucey0",
    "colab_type": "text"
   },
   "source": [
    "We set the learning rate to 1e-5 to ensure that the step size is small enough to let the network converge."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ypt0BWixxg2x",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "params_to_update = seqnet.parameters()\n",
    "optimizer_ft = AdamW(params_to_update, lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(size_average=True)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MVvs9mf6H0H",
    "colab_type": "text"
   },
   "source": [
    "Before we start training our model, let us quickly as a last control mechanism check if the split of the train and validation split was done such that the distribution of the class labels is roughly the same in both."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OUqIX8Gq6TaJ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "counts_train = np.unique(label_train, return_counts=True)[1]\n",
    "counts_val = np.unique(label_val, return_counts=True)[1]\n",
    "(counts_train, counts_val)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9itwU_36U3Q",
    "colab_type": "text"
   },
   "source": [
    "That is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71FLygeZEpOO",
    "colab_type": "text"
   },
   "source": [
    "5. Having done all of this we are good to go and can train our model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R3p5POrtE9ri",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "num_epochs=500\n",
    "seqnet_fit, hist = train_model(seqnet, data_loaders_dict, \n",
    "                             criterion, optimizer_ft, \n",
    "                             num_epochs=num_epochs)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usgRO8V8D5I9",
    "colab_type": "text"
   },
   "source": [
    "6. The validation accuracy looks promosing. However to get an idea of how representative that score is for the performance on the test set we will now predict the values for the test set and create a submission from those predictions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0HaFGld4LiRg",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "new_test_data = ae.encoder(torch.cuda.FloatTensor(np.array(test))).detach().cpu().numpy()\n",
    "new_test_data"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3JleaT-WrLA3",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#x_test = np.concatenate((np.array(test), new_test_data), axis=1)\n",
    "test_standardized = sc.transform(np.array(test))\n",
    "test_set = torch.from_numpy(np.array(test_standardized))\n",
    "predictions = []\n",
    "\n",
    "for value in test_set:\n",
    "  # then put it on the GPU, make it float and insert a fake batch dimension\n",
    "  test_value = Variable(value.cuda())\n",
    "  test_value = test_value.float()\n",
    "  test_value = test_value.unsqueeze(0)\n",
    "\n",
    "  # pass it through the model\n",
    "  outputs = seqnet_fit(test_value)\n",
    "  _, preds = torch.max(outputs, 1)\n",
    "\n",
    "# get the result out and reshape it\n",
    "  predictions.append(preds.cpu().numpy()[0])\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "predictions"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QXoPXHoszO2",
    "colab_type": "text"
   },
   "source": [
    "Let us quickly check the fitted distribution of the class labels."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gv0Mh52SrVje",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "unique, counts = np.unique(predictions, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMDMAawFtFmS",
    "colab_type": "text"
   },
   "source": [
    "Well that looks quite nice, recalling the distribution of the labels of our training and validation data. As we assume that the data for which we predicted the labels comes from the same distribution as the one which generated our training and validation data, this is exactly what we would expect.\n",
    "\n",
    "Let us now finally create a submission based on those predictions and download it, such that we potentially could hand it in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdXG2idCxHFm",
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "### Create submission"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Yor2Bbzsafyq",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "submission['y'] = predictions\n",
    "submission.head()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wD_P3gKvc9NG",
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "\n",
    "## Export data\n",
    "\n",
    "We finally use the Google Colab API to download our submission data frame in from of an csv, that we can submit to the submission platform."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WNhRwnu7djcg",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from google.colab import files\n",
    "\n",
    "ts = str(datetime.datetime.utcnow())\n",
    "ts = ts.replace(' ', '_')\n",
    "Filename = 'submission_name' #@param {type:\"string\"}\n",
    "fname = Filename+ts+'.csv'\n",
    "\n",
    "with open(fname, 'w') as f:\n",
    "  submission.to_csv(f, float_format='%.64f', index=True, header=True)\n",
    "\n",
    "files.download(fname)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HLkSFesU77_P",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}